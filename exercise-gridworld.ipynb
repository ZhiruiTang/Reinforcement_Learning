{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Introduction](#Introduction) \n",
    "* [Requirements](#Requirements) \n",
    "  * [Knowledge](#Knowledge) \n",
    "  * [Prerequisites](#Prerequisites) \n",
    "* [Python Modules](#Python-Modules)\n",
    "* [Exercises](#Exercises) \n",
    "* [Literature](#Literature) \n",
    "* [Licenses](#Licenses) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook you will train an agent to find a finish grid in a 4x4 Gridworld. This fairly easy examble will demonstrate the basic princibles of reinforcement learning in an agent-enviroment interaction system based on the Bellman equation. Along the exercises you will define all parts of the Bellman equation and in the last step solve this equation and therefore train an agent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "### Knowledge\n",
    "\n",
    "To solve this notebook you should aquire knowledge about fMDP (finite Markov Decision Processe), agent-enviroment interaction, Bellman equation and the policy iteration algorithm to solve the Bellman equation. \n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Read `SUT18`  chapter 3 and 4 to gain knowledge about the mentioned topics and terms. `SUT18` is the standard literature for reinforcment learning and the basis for this and following notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from hashlib import sha1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridworld\n",
    "\n",
    "The gridworld we consider here is a simple 4x4 grid with a possible start state $S$ and a fixed endstate $E$ in the bottom right corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gridworld\n",
      "\n",
      " -- -- -- --\n",
      "|S |  |  |  |\n",
      " -- -- -- --\n",
      "|  |  |  |  |\n",
      " -- -- -- --\n",
      "|  |  |  |  |\n",
      " -- -- -- --\n",
      "|  |  |  |E |\n",
      " -- -- -- --\n"
     ]
    }
   ],
   "source": [
    "print(\"Gridworld\")\n",
    "print()\n",
    "print(\" -- -- -- --\")\n",
    "print(\"|S |  |  |  |\")\n",
    "print(\" -- -- -- --\")\n",
    "print(\"|  |  |  |  |\")\n",
    "print(\" -- -- -- --\")\n",
    "print(\"|  |  |  |  |\")\n",
    "print(\" -- -- -- --\")\n",
    "print(\"|  |  |  |E |\")\n",
    "print(\" -- -- -- --\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States\n",
    "\n",
    "A 4x4 grid gives 16 states in total, which we can numerate from 1 to 16. As we will see later these will be the matrix rows, respectively columns when we define our enviroment behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- -- -- --\n",
      "|1 |2 |3 |4 |\n",
      " -- -- -- --\n",
      "|5 |6 |7 |8 |\n",
      " -- -- -- --\n",
      "|9 |10|11|12|\n",
      " -- -- -- --\n",
      "|13|14|15|16|\n",
      " -- -- -- --\n"
     ]
    }
   ],
   "source": [
    "print(\" -- -- -- --\")\n",
    "print(\"|1 |2 |3 |4 |\")\n",
    "print(\" -- -- -- --\")\n",
    "print(\"|5 |6 |7 |8 |\")\n",
    "print(\" -- -- -- --\")\n",
    "print(\"|9 |10|11|12|\")\n",
    "print(\" -- -- -- --\")\n",
    "print(\"|13|14|15|16|\")\n",
    "print(\" -- -- -- --\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action\n",
    "\n",
    "The actions our agent can take are **up, down, right and left**. Later this action will be encoded with numpy array indices **0,1,2 and 3**. Moving against a wall will leave the agent on his state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov decision process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a first inside into our envirement, in the next few exercises you will build transition matrices which contain the behavior of our model/world the agent is moving in.\n",
    "For that consider a random policy , meaning the agent will take any action with a probability of 25%. As said before hitting a wall will leave the agent in the actual state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "#### Task 1 \n",
    "\n",
    "Consider each state as a vector with start state given by (1,...,0), so we are with probability 1 in state 1. Find the transition matrix given a random policy for that given 4x4 gridworld. \n",
    "\n",
    "Here the endstate **doesn't** matter, so the agent there will behave the same as in any other state, we are not learning anything yet. \n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Each row represents the current state and each column a possible furture state.\n",
    "Your matrix should have shape (16,16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.array([\n",
    "    [0.5, 0.25, 0., 0., 0.25, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "    [0.25, 0.25, 0.25, 0., 0., 0.25, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "    [0., 0.25, 0.25, 0.25, 0., 0., 0.25, 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "    [0., 0., 0.25, 0.5, 0., 0., 0., 0.25, 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "    [0.25, 0., 0., 0., 0.25, 0.25, 0., 0., 0.25, 0., 0., 0., 0., 0., 0., 0.],\n",
    "    [0., 0.25, 0., 0., 0.25, 0., 0.25, 0., 0., 0.25, 0., 0., 0., 0., 0., 0.],\n",
    "    [0., 0., 0.25, 0., 0., 0.25, 0., 0.25, 0., 0., 0.25, 0., 0., 0., 0., 0.],\n",
    "    [0., 0., 0., 0.25, 0., 0., 0.25, 0.25, 0., 0., 0., 0.25, 0., 0., 0., 0.],\n",
    "    [0., 0., 0., 0., 0.25, 0., 0., 0., 0.25, 0.25, 0., 0., 0.25, 0., 0., 0.],\n",
    "    [0., 0., 0., 0., 0., 0.25, 0., 0., 0.25, 0., 0.25, 0., 0., 0.25, 0., 0.],\n",
    "    [0., 0., 0., 0., 0., 0., 0.25, 0., 0., 0.25, 0., 0.25, 0., 0., 0.25, 0.],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0.25, 0., 0., 0.25, 0.25, 0., 0., 0., 0.25],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0.25, 0., 0., 0., 0.5, 0.25, 0., 0.],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.25, 0., 0., 0.25, 0.25, 0.25, 0.],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.25, 0., 0., 0.25, 0.25, 0.25],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.25, 0., 0., 0.25, 0.5]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your matrix here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_T = sha1(T).hexdigest()\n",
    "assert str(hash_T) == \"42b00d22d434f693a40dc196f0dee056383c6672\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "\n",
    "Taken two random steps , what is the probability to go to state 3 and what is the probability to go to state 6 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability to go to state 3 and state 6 are both 1/16\n"
     ]
    }
   ],
   "source": [
    "print('The probability to go to state 3 and state 6 are both 1/16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "\n",
    "If we do many steps, lets say 100+, what is the probability for each possible state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am not sure, but I guess they are all 1/16\n",
      "At first, the probabilties are unstable. But after many steps, they converge to 1/16\n"
     ]
    }
   ],
   "source": [
    "print('I am not sure, but I guess they are all 1/16')\n",
    "print('At first, the probabilties are unstable. But after many steps, they converge to 1/16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "\n",
    "Now lets say the agent is a bit more clever and let him \"know\" that the end is somewhere down to the right. So he only goes right by 50% and down by 50%.\n",
    "\n",
    "* Find the transition matrix.\n",
    "* At how many steps we can be sure we reached the end?\n",
    "* What would be our optimal solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_clever = np.array([\n",
    "    [0., 0.5, 0., 0., 0.5, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "    [0., 0., 0.5, 0., 0., 0.5, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "    [0., 0., 0., 0.5, 0., 0., 0.5, 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "    [0., 0., 0., 0.5, 0., 0., 0., 0.5, 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "    [0., 0., 0., 0., 0., 0.5, 0., 0., 0.5, 0., 0., 0., 0., 0., 0., 0.],\n",
    "    [0., 0., 0., 0., 0., 0., 0.5, 0., 0., 0.5, 0., 0., 0., 0., 0., 0.],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0.5, 0., 0., 0.5, 0., 0., 0., 0., 0.],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0.5, 0., 0., 0., 0.5, 0., 0., 0., 0.],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.5, 0., 0., 0.5, 0., 0., 0.],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.5, 0., 0., 0.5, 0., 0.],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.5, 0., 0., 0.5, 0.],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.5, 0., 0., 0., 0.5],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.5, 0.5, 0., 0.],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.5, 0.5, 0.],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.5, 0.5],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 6 steps we can get the end\n",
      "There are several optimal solutions. All of them keep going without 'hitting the wall'\n"
     ]
    }
   ],
   "source": [
    "print('With 6 steps we can get the end')\n",
    "print(\"There are several optimal solutions. All of them keep going without 'hitting the wall'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your matrix here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_T_clever = sha1(T_clever).hexdigest()\n",
    "assert str(hash_T_clever) == \"4597d1bfacf69ab6c021bc4f14e25469d96de872\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution with reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this gridworld problem with reinforcement learning we use the Bellman equation and policy iteration found on page 80 in `SUT18`.\n",
    "\n",
    "The Bellman equation is given by\n",
    "$$\n",
    "v_{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s'} \\sum_r p(s',r|s,a)[r + \\gamma v_\\pi(s')].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $p(s',r|s,a)$ (model) we have a 16x16x4 matrix , so a 16x16 transition matrix for each action (**up,down,right,left**) which we number (**0,1,2,3**). From the previous exercises we know how to set up these matrixes.\n",
    "\n",
    "**Hint:** For each action taken in a state the agent transitions into another state corresbonding to the action taken. So each row should have exactly one $1$ entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "\n",
    "Find for each action the corresbonding transition matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "#p(s', r|s, a=up)\n",
    "P_up = np.array([\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "])\n",
    "\n",
    "P_down = np.array([\n",
    "    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "P_right = np.array([\n",
    "    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "\n",
    "P_left = np.array([\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],   \n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "])\n",
    "\n",
    "P_model = np.array([P_up,P_down,P_left,P_right]) \n",
    "print(P_model.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your matrix here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_P_model = sha1(P_model).hexdigest()\n",
    "assert str(hash_P_model) == \"afd11e846b0d2622ab11dc1a5505077623ac9d5f\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward\n",
    "In order for the agent to learn we have to define a good reward strategy.\n",
    "A simple choice is  -1 for each transition to any state, which is not the end state, and +10 for end state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. 10.]\n"
     ]
    }
   ],
   "source": [
    "reward = -1*np.ones(16)\n",
    "reward[-1] = 10\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\gamma$  -value\n",
    "\n",
    "The $\\gamma$  -value defines how important future rewards are. A default choice for problems like gridworld is 0.9, so future rewards are important for the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots the values for all 16 states in our gridworld\n",
    "def show_values(V):\n",
    "    #print(\"Values\")\n",
    "    print()\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"|\",round(V[0],1),\"|\",round(V[1],1),\"|\",round(V[2],1),\"|\",round(V[3],1),\"|\")\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"|\",round(V[4],1),\"|\",round(V[5],1),\"|\",round(V[6],1),\"|\",round(V[7],1),\"|\")\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"|\",round(V[8],1),\"|\",round(V[9],1),\"|\",round(V[10],1),\"|\",round(V[11],1),\"|\")\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"|\",round(V[12],1),\"|\",round(V[13],1),\"|\",round(V[14],1),\"|\",round(V[15],1),\"|\")\n",
    "    print(\"-----------------------------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "\n",
    "Study the pseudo-algorithm for **Iterative Policy Evaluation** on page 75 in `SUT18` and implement\\complete the `policy_evaluation` function below for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(V_k,policy,P_model,reward,gamma):\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "    \n",
    "    index abbreviations:\n",
    "    \n",
    "    - state s\n",
    "    - action a\n",
    "    - next state s_nxt\n",
    "    \n",
    "    \n",
    "        V_k: current values, size 16 array V_k[s]\n",
    "        \n",
    "        policy: current policy, 16x4 ndarray policy[s,a]\n",
    "        \n",
    "        P_model: fixed model behaviour, 4x16x16 ndarray P_model[a,s,s_nxt]\n",
    "        \n",
    "        reward: fixed reward model, size 16 array reward[s_nxt]\n",
    "        \n",
    "        gamma: fixed discount rate , float\n",
    "    \n",
    "    Return:\n",
    "    \n",
    "        V_k: updated values (overwritten old values)\n",
    "    \"\"\"\n",
    "    #r is fixed, so only 1 layer loop instead of 2-layers loop\n",
    "    delta = 1e6\n",
    "    a = np.argmax(policy, axis=1)\n",
    "    while delta > 1e-6:\n",
    "        v = V_k\n",
    "        for s in range(len(V_k)):\n",
    "            V_k[s] = sum([P_model[a[s], s, s_nxt]*(reward[s_nxt]+gamma*v[s_nxt]) for s_nxt in range(len(V_k))])\n",
    "        delta = np.max(abs(V_k - v))\n",
    "        delta = max(delta, np.max(abs(V_k - v)))\n",
    "    return V_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your implementation, run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"If nothing happend you are good!\")\n",
    "# test_V_k = np.arange(16)\n",
    "# test_policy = 0.25*np.ones((16,4))\n",
    "# update_V_K = policy_evaluation(test_V_k,test_policy,P_model,reward,gamma)\n",
    "# hash_V_K = sha1(update_V_K).hexdigest()\n",
    "# assert str(hash_V_K) == \"ee70b34908511bc6e2b44f2fac8151d7933cf6e7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots the \"best\" action given by the actual policy for all 16 states in our gridworld\n",
    "def show_policy(p):\n",
    "    #print(\"Policy\")\n",
    "    print()\n",
    "    print(\"Actions with highest probability\")\n",
    "    print(\"_______\")\n",
    "    print(\"a := all possible actions\")\n",
    "    \n",
    "    \n",
    "    def symbol(a): \n",
    "        if (a[0]==a[1]) and (a[1]==a[2]) and (a[2]==a[3]):\n",
    "            return \"a\"\n",
    "        if np.argmax(a)==0:\n",
    "            return \"↑\"\n",
    "        if np.argmax(a)==1:\n",
    "            return \"↓\"\n",
    "        if np.argmax(a)==2:\n",
    "            return \"←\"\n",
    "        if np.argmax(a)==3:\n",
    "            return \"→\"\n",
    "        \n",
    "    print(\"-----------------\")\n",
    "    print(\"| \"+symbol(p[0])+\" | \"+symbol(p[1])+\" | \"+symbol(p[2])+\" | \"+symbol(p[3])+\" |\")\n",
    "    print(\"-----------------\")\n",
    "    print(\"| \"+symbol(p[4])+\" | \"+symbol(p[5])+\" | \"+symbol(p[6])+\" | \"+symbol(p[7])+\" |\")\n",
    "    print(\"-----------------\")\n",
    "    print(\"| \"+symbol(p[8])+\" | \"+symbol(p[9])+\" | \"+symbol(p[10])+\" | \"+symbol(p[11])+\" |\")\n",
    "    print(\"-----------------\")\n",
    "    print(\"| \"+symbol(p[12])+\" | \"+symbol(p[13])+\" | \"+symbol(p[14])+\" |>E<|\")\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 7\n",
    "\n",
    "Study the pseudo-algorithm for **Policy Improvement** on page 80 in `SUT18` and implement\\complete the `policy_improvement` function below for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(V_k,policy,P_model,reward,gamma):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    \n",
    "    index abbreviations:\n",
    "    \n",
    "    - state s\n",
    "    - action a\n",
    "    - next state s_nxt\n",
    "    \n",
    "    \n",
    "        V_k: current values, size 16 array V_k[s]\n",
    "        \n",
    "        policy: current policy, 16x4 ndarray policy[s,a]\n",
    "        \n",
    "        P_model: fixed model behaviour, 16x16x4 ndarray P_model[a,s,s_nxt]\n",
    "        \n",
    "        reward: fixed reward model, size 16 array reward[s_nxt]\n",
    "        \n",
    "        gamma: fixed discount rate , float\n",
    "    \n",
    "    Return:\n",
    "    \n",
    "        policy: updated policy (overwritten old policy)\n",
    "    \"\"\"\n",
    "    \n",
    "    for s in range(policy.shape[0]):\n",
    "        for a in range(policy.shape[1]):\n",
    "            policy[s,a] = sum([P_model[a, s, s_nxt]*(reward[s_nxt]+gamma*V_k[s_nxt]) for s_nxt in range(len(V_k))])\n",
    "            \n",
    "    return policy\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your implementation, run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"If nothing happend you are good!\")\n",
    "# test_V_k = np.arange(16)\n",
    "# test_policy = 0.25*np.ones((16,4))\n",
    "# update_policy = policy_improvement(test_V_k,test_policy,P_model,reward,gamma)\n",
    "# hash_policy = sha1(update_policy).hexdigest()\n",
    "# assert str(hash_policy) == \"5e0dfa1ae264b0d7d829bb96d55bd9bf87b932d3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Values and random Policy for training. \n",
    "\n",
    "Here we initialize and show the starting values and policy our agent uses to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------\n",
      "| 0.9 | 0.5 | 0.6 | 0.5 |\n",
      "-----------------------------\n",
      "| 0.9 | 0.5 | 0.2 | 1.0 |\n",
      "-----------------------------\n",
      "| 0.1 | 0.1 | 0.1 | 0.5 |\n",
      "-----------------------------\n",
      "| 0.2 | 0.1 | 0.0 | 0.1 |\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "Actions with highest probability\n",
      "_______\n",
      "a := all possible actions\n",
      "-----------------\n",
      "| → | → | → | ← |\n",
      "-----------------\n",
      "| → | ↓ | ↑ | ← |\n",
      "-----------------\n",
      "| ← | ↑ | → | ↓ |\n",
      "-----------------\n",
      "| ↑ | ↑ | ← |>E<|\n",
      "-----------------\n",
      "\n",
      "[[0.26932033 0.32498877 0.02305311 0.38263779]\n",
      " [0.25123393 0.37039628 0.00738458 0.37098521]\n",
      " [0.24853615 0.22741501 0.23012126 0.29392758]\n",
      " [0.21172657 0.06266553 0.55353906 0.17206884]\n",
      " [0.34131982 0.08448365 0.21950542 0.35469111]\n",
      " [0.12681998 0.61855507 0.1415013  0.11312365]\n",
      " [0.33567901 0.17599959 0.19259559 0.29572581]\n",
      " [0.05851828 0.26538316 0.36183154 0.31426702]\n",
      " [0.29545355 0.34092005 0.35528801 0.00833839]\n",
      " [0.30169993 0.28932468 0.16173275 0.24724264]\n",
      " [0.23125067 0.31261911 0.10219704 0.35393319]\n",
      " [0.30204862 0.52210265 0.02529322 0.15055551]\n",
      " [0.34428153 0.11672365 0.21526163 0.32373319]\n",
      " [0.35732839 0.14060452 0.18297478 0.31909231]\n",
      " [0.18590273 0.35757642 0.40324296 0.05327789]\n",
      " [0.32861992 0.25693337 0.33949047 0.07495624]]\n"
     ]
    }
   ],
   "source": [
    "#V_k = np.zeros(16,dtype=float)\n",
    "policy = 0.25*np.ones((16,4))\n",
    "\n",
    "V_k = np.random.rand(16)\n",
    "show_values(V_k)\n",
    "\n",
    "# generate complete random policy\n",
    "for s in range(16):\n",
    "    act = np.random.rand(4)\n",
    "    policy[s] = act/sum(act)\n",
    "    \n",
    "show_policy(policy)\n",
    "print()\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration loop\n",
    "\n",
    "Run the following code to train the agend. This loop is the full **policy iteration algorithm** given on page 80 in `SUT18`. If your implementations above work you should the see learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Iteration 0---------------\n",
      "\n",
      "\n",
      "-----------------------------\n",
      "| -0.5 | -0.5 | -0.6 | -1.5 |\n",
      "-----------------------------\n",
      "| -0.6 | -0.9 | -1.5 | -2.4 |\n",
      "-----------------------------\n",
      "| -0.9 | -1.8 | -0.6 | 10.1 |\n",
      "-----------------------------\n",
      "| -1.8 | -2.6 | -3.4 | -4.0 |\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "Actions with highest probability\n",
      "_______\n",
      "a := all possible actions\n",
      "-----------------\n",
      "| → | ↑ | ← | ← |\n",
      "-----------------\n",
      "| ↑ | ↑ | ↓ | ↓ |\n",
      "-----------------\n",
      "| ↑ | → | → | → |\n",
      "-----------------\n",
      "| ↑ | ↑ | → |>E<|\n",
      "-----------------\n",
      "\n",
      "------------ Iteration 1---------------\n",
      "\n",
      "\n",
      "-----------------------------\n",
      "| -1.4 | -1.4 | -2.3 | -3.1 |\n",
      "-----------------------------\n",
      "| -2.3 | -2.3 | -1.5 | 8.1 |\n",
      "-----------------------------\n",
      "| -3.1 | -1.5 | 8.1 | 8.1 |\n",
      "-----------------------------\n",
      "| -3.8 | -2.3 | 6.4 | 6.3 |\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "Actions with highest probability\n",
      "_______\n",
      "a := all possible actions\n",
      "-----------------\n",
      "| ↑ | ↑ | ← | ↓ |\n",
      "-----------------\n",
      "| ↑ | ↑ | ↓ | ↓ |\n",
      "-----------------\n",
      "| → | → | → | ↓ |\n",
      "-----------------\n",
      "| → | → | → |>E<|\n",
      "-----------------\n",
      "\n",
      "------------ Iteration 2---------------\n",
      "\n",
      "\n",
      "-----------------------------\n",
      "| -2.3 | -2.3 | -3.1 | 6.3 |\n",
      "-----------------------------\n",
      "| -3.1 | -3.1 | 6.3 | 6.3 |\n",
      "-----------------------------\n",
      "| -2.3 | 6.3 | 6.3 | 15.6 |\n",
      "-----------------------------\n",
      "| -3.1 | 4.7 | 15.6 | 15.6 |\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "Actions with highest probability\n",
      "_______\n",
      "a := all possible actions\n",
      "-----------------\n",
      "| ↑ | ↑ | ↓ | ↑ |\n",
      "-----------------\n",
      "| ↑ | ↓ | ↓ | ↓ |\n",
      "-----------------\n",
      "| → | → | ↓ | ↓ |\n",
      "-----------------\n",
      "| → | → | → |>E<|\n",
      "-----------------\n",
      "\n",
      "------------ Iteration 3---------------\n",
      "\n",
      "\n",
      "-----------------------------\n",
      "| -3.1 | -3.1 | 4.6 | 4.6 |\n",
      "-----------------------------\n",
      "| -3.8 | 4.6 | 4.6 | 13.1 |\n",
      "-----------------------------\n",
      "| 4.6 | 4.6 | 13.1 | 24.1 |\n",
      "-----------------------------\n",
      "| 3.3 | 13.1 | 24.1 | 24.1 |\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "Actions with highest probability\n",
      "_______\n",
      "a := all possible actions\n",
      "-----------------\n",
      "| ↑ | ↓ | ↑ | ↓ |\n",
      "-----------------\n",
      "| ↓ | ↓ | ↓ | ↓ |\n",
      "-----------------\n",
      "| ← | ↓ | ↓ | ↓ |\n",
      "-----------------\n",
      "| → | → | → |>E<|\n",
      "-----------------\n",
      "\n",
      "------------ Iteration 4---------------\n",
      "\n",
      "\n",
      "-----------------------------\n",
      "| -3.8 | 3.2 | 3.2 | 10.8 |\n",
      "-----------------------------\n",
      "| 3.2 | 3.2 | 10.8 | 20.7 |\n",
      "-----------------------------\n",
      "| 3.2 | 10.8 | 20.7 | 31.7 |\n",
      "-----------------------------\n",
      "| 10.8 | 20.7 | 31.7 | 31.7 |\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "Actions with highest probability\n",
      "_______\n",
      "a := all possible actions\n",
      "-----------------\n",
      "| ↓ | ↑ | ↓ | ↓ |\n",
      "-----------------\n",
      "| ↓ | ↓ | ↓ | ↓ |\n",
      "-----------------\n",
      "| ↓ | ↓ | ↓ | ↓ |\n",
      "-----------------\n",
      "| → | → | → |>E<|\n",
      "-----------------\n",
      "\n",
      "------------ Iteration 5---------------\n",
      "\n",
      "\n",
      "-----------------------------\n",
      "| 1.9 | 1.9 | 8.7 | 17.6 |\n",
      "-----------------------------\n",
      "| 1.9 | 8.7 | 17.6 | 27.5 |\n",
      "-----------------------------\n",
      "| 8.7 | 17.6 | 27.5 | 38.5 |\n",
      "-----------------------------\n",
      "| 17.6 | 27.5 | 38.5 | 38.5 |\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "Actions with highest probability\n",
      "_______\n",
      "a := all possible actions\n",
      "-----------------\n",
      "| a | ↓ | ↓ | ↓ |\n",
      "-----------------\n",
      "| ↓ | ↓ | ↓ | ↓ |\n",
      "-----------------\n",
      "| ↓ | ↓ | ↓ | ↓ |\n",
      "-----------------\n",
      "| → | → | → |>E<|\n",
      "-----------------\n",
      "\n",
      "------------ Iteration 6---------------\n",
      "\n",
      "\n",
      "-----------------------------\n",
      "| 0.7 | 6.8 | 14.8 | 23.7 |\n",
      "-----------------------------\n",
      "| 6.8 | 14.8 | 23.7 | 33.6 |\n",
      "-----------------------------\n",
      "| 14.8 | 23.7 | 33.6 | 44.6 |\n",
      "-----------------------------\n",
      "| 23.7 | 33.6 | 44.6 | 44.6 |\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "Actions with highest probability\n",
      "_______\n",
      "a := all possible actions\n",
      "-----------------\n",
      "| ↓ | ↓ | ↓ | ↓ |\n",
      "-----------------\n",
      "| ↓ | ↓ | ↓ | ↓ |\n",
      "-----------------\n",
      "| ↓ | ↓ | ↓ | ↓ |\n",
      "-----------------\n",
      "| → | → | → |>E<|\n",
      "-----------------\n",
      "\n",
      "------------ Iteration 7---------------\n",
      "\n",
      "\n",
      "-----------------------------\n",
      "| 5.1 | 12.4 | 20.4 | 29.3 |\n",
      "-----------------------------\n",
      "| 12.4 | 20.4 | 29.3 | 39.2 |\n",
      "-----------------------------\n",
      "| 20.4 | 29.3 | 39.2 | 50.2 |\n",
      "-----------------------------\n",
      "| 29.3 | 39.2 | 50.2 | 50.2 |\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "Actions with highest probability\n",
      "_______\n",
      "a := all possible actions\n",
      "-----------------\n",
      "| ↓ | ↓ | ↓ | ↓ |\n",
      "-----------------\n",
      "| ↓ | ↓ | ↓ | ↓ |\n",
      "-----------------\n",
      "| ↓ | ↓ | ↓ | ↓ |\n",
      "-----------------\n",
      "| → | → | → |>E<|\n",
      "-----------------\n",
      "\n",
      "------------ Iteration 8---------------\n",
      "\n",
      "\n",
      "-----------------------------\n",
      "| 10.1 | 17.3 | 25.4 | 34.3 |\n",
      "-----------------------------\n",
      "| 17.3 | 25.4 | 34.3 | 44.2 |\n",
      "-----------------------------\n",
      "| 25.4 | 34.3 | 44.2 | 55.2 |\n",
      "-----------------------------\n",
      "| 34.3 | 44.2 | 55.2 | 55.2 |\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "Actions with highest probability\n",
      "_______\n",
      "a := all possible actions\n",
      "-----------------\n",
      "| ↓ | ↓ | ↓ | ↓ |\n",
      "-----------------\n",
      "| ↓ | ↓ | ↓ | ↓ |\n",
      "-----------------\n",
      "| ↓ | ↓ | ↓ | ↓ |\n",
      "-----------------\n",
      "| → | → | → |>E<|\n",
      "-----------------\n",
      "\n",
      "------------ Iteration 9---------------\n",
      "\n",
      "\n",
      "-----------------------------\n",
      "| 14.6 | 21.8 | 29.8 | 38.7 |\n",
      "-----------------------------\n",
      "| 21.8 | 29.8 | 38.7 | 48.6 |\n",
      "-----------------------------\n",
      "| 29.8 | 38.7 | 48.6 | 59.6 |\n",
      "-----------------------------\n",
      "| 38.7 | 48.6 | 59.6 | 59.6 |\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "Actions with highest probability\n",
      "_______\n",
      "a := all possible actions\n",
      "-----------------\n",
      "| ↓ | ↓ | ↓ | ↓ |\n",
      "-----------------\n",
      "| ↓ | ↓ | ↓ | ↓ |\n",
      "-----------------\n",
      "| ↓ | ↓ | ↓ | ↓ |\n",
      "-----------------\n",
      "| → | → | → |>E<|\n",
      "-----------------\n",
      "\n",
      "------------ Iteration 10---------------\n",
      "\n",
      "\n",
      "-----------------------------\n",
      "| 18.6 | 25.9 | 33.9 | 42.8 |\n",
      "-----------------------------\n",
      "| 25.9 | 33.9 | 42.8 | 52.7 |\n",
      "-----------------------------\n",
      "| 33.9 | 42.8 | 52.7 | 63.7 |\n",
      "-----------------------------\n",
      "| 42.8 | 52.7 | 63.7 | 63.7 |\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "Actions with highest probability\n",
      "_______\n",
      "a := all possible actions\n",
      "-----------------\n",
      "| ↓ | ↓ | ↓ | ↓ |\n",
      "-----------------\n",
      "| ↓ | ↓ | ↓ | ↓ |\n",
      "-----------------\n",
      "| ↓ | ↓ | ↓ | ↓ |\n",
      "-----------------\n",
      "| → | → | → |>E<|\n",
      "-----------------\n",
      "--> Training Done!\n",
      "\n",
      "Final policy\n",
      "\n",
      "[[15.77271567 22.26810567 15.77271567 22.26810567]\n",
      " [22.26810567 29.48520567 15.77271567 29.48520567]\n",
      " [29.48520567 37.50420567 22.26810567 37.50420567]\n",
      " [37.50420567 46.41420567 29.48520567 37.50420567]\n",
      " [15.77271567 29.48520567 22.26810567 29.48520567]\n",
      " [22.26810567 37.50420567 22.26810567 37.50420567]\n",
      " [29.48520567 46.41420567 29.48520567 46.41420567]\n",
      " [37.50420567 56.31420567 37.50420567 46.41420567]\n",
      " [22.26810567 37.50420567 29.48520567 37.50420567]\n",
      " [29.48520567 46.41420567 29.48520567 46.41420567]\n",
      " [37.50420567 56.31420567 37.50420567 56.31420567]\n",
      " [46.41420567 67.31420567 46.41420567 56.31420567]\n",
      " [29.48520567 37.50420567 37.50420567 46.41420567]\n",
      " [37.50420567 46.41420567 37.50420567 56.31420567]\n",
      " [46.41420567 56.31420567 46.41420567 67.31420567]\n",
      " [56.31420567 67.31420567 56.31420567 67.31420567]]\n"
     ]
    }
   ],
   "source": [
    "policy_stable = False\n",
    "max_loops = 10\n",
    "ite = 0 \n",
    "\n",
    "while not policy_stable:\n",
    "    \n",
    "    print(\"------------ Iteration \"+str(ite)+\"---------------\")\n",
    "    print()\n",
    "    \n",
    "    old_policy = copy.copy(policy)\n",
    "    \n",
    "    V_k = policy_evaluation(V_k,policy,P_model,reward,gamma)\n",
    "    policy = policy_improvement(V_k,policy,P_model,reward,gamma)\n",
    "    \n",
    "    show_values(V_k)\n",
    "    show_policy(policy)\n",
    "    \n",
    "    # check if stable\n",
    "    policy_stable = np.all(policy==old_policy)\n",
    "        \n",
    "    ite += 1\n",
    "    if ite > max_loops:\n",
    "        break\n",
    "    print()\n",
    "\n",
    "print(\"--> Training Done!\")\n",
    "print()\n",
    "print(\"Final policy\")\n",
    "print()\n",
    "print(policy)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <a name=\"SUT18\"></a>[SUT18]\n",
    "        </td>\n",
    "        <td>\n",
    "            Richard S. Sutton and Andrew G. Barto, “Reinforcement Learning: An Introduction\n",
    "” ,2nd edition,2018. \n",
    "        </td>\n",
    "    </tr>\n",
    "   \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Licenses\n",
    "\n",
    "### Notebook License (CC-BY-SA 4.0)\n",
    "\n",
    "*The following license applies to the complete notebook, including code cells. It does however not apply to any referenced external media (e.g., images).*\n",
    "\n",
    "Exercise Gridworld\n",
    "\n",
    "Oliver Fischer\n",
    "\n",
    "is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/).<br/>\n",
    "Based on a work at https://gitlab.com/deep.TEACHING.\n",
    "\n",
    "\n",
    "### Code License (MIT)\n",
    "\n",
    "*The following license only applies to code cells of the notebook.*\n",
    "\n",
    "Copyright 2019  Oliver Fischer\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
