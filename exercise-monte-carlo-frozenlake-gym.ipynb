{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Monte Carlo with Frozenlake from gym API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Introduction](#Introduction) \n",
    "* [Requirements](#Requirements) \n",
    "  * [Knowledge](#Knowledge) \n",
    "  * [Prerequisites](#Prerequisites) \n",
    "* [Python Modules](#Python-Modules)\n",
    "* [Frozenlake enviroment](#Frozenlake-enviroment)\n",
    "* [Exercises](#Exercises) \n",
    "* [Appendix](#Appendix)\n",
    "* [Literature](#Literature) \n",
    "* [Licenses](#Licenses) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this exercise you will learn techniques based on Monte Carlo estimators to solve reinforcement learning problems in which you don't know the environmental behavior. This stands in contrast to the gridworld examble seen before, where the full behavior of the environment was known and could be modeled. Here we have to learn based on an episode by episode strategy and estimate the state-action values over many episodes to find an optimal/good policy. As an examble for this we consider the frozenlake environment provided by the gym API. The fozenlake environment is represented by a 4x4 grid consisting of a start grid , some hole grids and one goal grid. As in the gridworld examble the agent can move, up, down, right and left. As further difficulty the grid is also slippery, meaning that an action might not lead to the desired direction. The rewards are set as follows, 0 for each transition not entering the goal grid and +1 for reaching the goal grid.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "### Knowledge\n",
    "\n",
    "To solve this notebook you should aquire knowledge about Monte Carlo methods, \n",
    "agent-environment interaction, state-action values and policy improvement.\n",
    "### Prerequisites\n",
    "\n",
    "Read `SUT18`  chapter 5 to gain knowledge about the mentioned topics and terms. \n",
    "`SUT18` is the standard literature for reinforcment learning and the basis for this and following notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozenlake environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you will find some basic commands on how to work with the frozenlake environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the environment\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sizes\")\n",
    "print(\"-----\")\n",
    "print(\"Action space: \", env.action_space)\n",
    "print(\"Observation space: \", env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset() #reset the environment the set agent to start state\n",
    "env.render() #display the agents current state on the grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The abbreviations stand for\n",
    "\n",
    "- `S`: Start\n",
    "- `F`: Frozen (safe)\n",
    "- `H`: Hole\n",
    "- `G`: Goal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actions are numerated as **left (0), right (1), down (2), up (3)**.\n",
    "\n",
    "A generic random walk until a terminal state is reached (done == True) is implemented below.\n",
    "\n",
    "Such a walk, until a terminal state is reached, represents one episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for i in range(10):\n",
    "    random_action = env.action_space.sample() #samples a random action\n",
    "    print(\"Action:\",random_action)\n",
    "    new_state, reward, done, info = env.step(random_action) #agent takes action (random_action)\n",
    "    \n",
    "    \"\"\"\n",
    "        new_state: new state after action (random_action) taken in current state \n",
    "        reward: reward obtained after taken action (random_action) in current state and entering new state (new_state)\n",
    "        done: bool flag, true if goal or hole is reached\n",
    "        info: slippery probability, baseline is 1/3\n",
    "    \n",
    "    \"\"\"\n",
    "    env.render() # display current agent state\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The states are internally enumerate as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "print(\" -- -- -- --\")\n",
    "print(\"|1 |2 |3 |4 |\")\n",
    "print(\" -- -- -- --\")\n",
    "print(\"|5 |6 |7 |8 |\")\n",
    "print(\" -- -- -- --\")\n",
    "print(\"|9 |10|11|12|\")\n",
    "print(\" -- -- -- --\")\n",
    "print(\"|13|14|15|16|\")\n",
    "print(\" -- -- -- --\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminal states are 6,8,12,13 (Holes) and 16 (Goal).\n",
    "The start state is always 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing a policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is said to be solved, a good policy found, if over a sufficient number of episodes (>100) a  mean reward of >0.7 is reached. The code below tests this for a given policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_performance(policy, nb_episodes=100):\n",
    "    sum_returns = 0\n",
    "    for i in range(nb_episodes):\n",
    "        state  = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                sum_returns += reward\n",
    "    return sum_returns/nb_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets set and test a random policy. Since later we will work with a $\\epsilon-policy$ we always will start by defining a policy as a dictionary (state-action pairs) or array (index-value pairs) and than wrap the dictionary or array into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_dict = {0:1, 1:2, 2:1, 3:0, 4:1, 6:1, 8:2, 9:0, 10:1, 13:2, 14:2} #random policy\n",
    "policy = lambda s: policy_dict[s]\n",
    "\n",
    "# calling dictionary with [] and function with ()\n",
    "\n",
    "print(\"Mean reward:\",test_performance(policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the number is far lower than 0.7 and therefor the policy is not good/ optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo (every visit) with $\\epsilon$ policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the following exercies is to set up a learning algorihtm using a **Monte Carlo method** based on the **every visit** estimate for the state-action values combined with an $\\epsilon$ policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_dict = {0:1, 1:2, 2:1, 3:0, 4:1, 6:1, 8:2, 9:0, 10:1, 13:2, 14:2} #random\n",
    "policy = lambda s: policy_dict[s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Since Monte Carlo methods are based on learning from episodes write a function `random_episode` that generates an eposide given the frozenlake environment and a policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_episode(env,policy):\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        env: given enviroment, here frozenlake\n",
    "        policy: certain policy for the agent \n",
    "        \n",
    "    Return:\n",
    "        episode: list of (state,action, reward) tribles\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "As said before we will work with a policy dictionary or array wraped in a function. Write a function `epsilon_policy` that wraps a policy dictionary or array into an $\\epsilon - policy$ function ($\\epsilon-greedy-policy$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_policy(policy_dict, epsilon=0.2, env=env):\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        policy_dict: policy dictionary\n",
    "        epsilon: epsilon paramter, decision boundary for exploration vs explotation\n",
    "        env: enviroment\n",
    "        \n",
    "    Return: \n",
    "        epsilon_greedy_policy \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def epsilon_greedy_policy(s):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s: state s\n",
    "            \n",
    "        Return:\n",
    "            a: action based on an epsilon greedy method using the policy dict\n",
    "        \n",
    "        \"\"\"\n",
    "        return NotImplementedError\n",
    "      \n",
    "    \n",
    "    return epsilon_greedy_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tie-breaking argmax\n",
    "\n",
    "As mentioned in the algorithms in `SUT18` it is important to use a tie-breaking argmax. The numpy argmax naturally will only find the first biggest number in an array and therefore given a certain q-table prefers specific actions just based on their position, respectively numeration inside that q-table. To avoid this problem use the `random_argmax_axis1` function given below for your implementation instead of the numpy argmax to improve learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_argmax_axis1(b):\n",
    "    \"\"\" a random tie-breaking argmax\"\"\"\n",
    "    return np.argmax(np.random.random(b.shape) * (b.T==b.max(axis=1)).T, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "`SUT18` provides many Monte Carlo based learning algorithm techniques including, exploring starts (ES), $\\epsilon$-greedy, first-visit and every-visit estimation. Why are ES and first-visit not well suited for the frozenlake environment? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Write a function `MC_epsilon_greedy_every_visit` that performs policy improvement based on Monte Carlo every vist estimation for the state-action values and use the `epsilon_policy` to wrap the policy dictionary/ array.\n",
    "\n",
    "**Caution:**\n",
    "If you follow the pseudo code algorithms presented in `SUT18` it is important you run reverse through your generated episodes, see [Appendix](#Appendix) task 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def MC_epsilon_greedy_every_visit(env,nb_eps,policy_dict,gamma=0.95):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        env: enviroment\n",
    "        nb_eps: number of episodes used to train\n",
    "        policy_dict: policy dictionary\n",
    "    Kwargs:\n",
    "        gamma: discount rate\n",
    "    \n",
    "    Returns:\n",
    "        policy: policy as a function\n",
    "        greedy_policy: policy as array, final policy to test \n",
    "        Q_sa: q-table\n",
    "        returns_Q_sa: dictionary for states-actions paris with cumulative return and number of visits information\n",
    "    \n",
    "    \"\"\"\n",
    "    # state-action values Q(s,a) for 16 states and 4 actions, q-table\n",
    "    Q_sa = np.zeros((16,4))\n",
    "    \n",
    "    # set dictionary for (s,a) pairs [keys], cumulative return and number of visits (return_sum,nb_visits) [values]\n",
    "    returns_Q_sa = dict()\n",
    "    \n",
    "    # get epsilon policy \n",
    "    policy = epsilon_policy(policy_dict)\n",
    "    \n",
    "    #for i in range(nb_eps):\n",
    "        # get episode\n",
    "        \n",
    "        #for e in reversed(episode):\n",
    "            # calculated reward and number of visits \n",
    "            # add to dictionary\n",
    "            # calculate q-table\n",
    "            \n",
    "        \n",
    "        \n",
    "        # find current best policy with argmax\n",
    "\n",
    "        # set greedy policy \n",
    "        \n",
    "    return NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Execute the cell below to train the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_dict = {0:1, 1:2, 2:1, 3:0, 4:1, 6:1, 8:2, 9:0, 10:1, 13:2, 14:2} # initialize random policy\n",
    "policy,greedy_policy,Q_sa,returns_Q_sa = MC_epsilon_greedy_every_visit(env,100000,policy_dict=policy_dict,gamma=0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(greedy_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test performance after training\n",
    "\n",
    "Since each performance test might lead to a different value accordingly to the random nature of our frozenlake environment due to its slippery condition, it is usefull to find the mean over many tests. Reaching over 0.7 means your learning was succesful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = lambda s: greedy_policy[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_performance_list = []\n",
    "mean_performance = 0\n",
    "n = 300\n",
    "for i in range(n):\n",
    "    performance = test_performance(policy)\n",
    "    mean_performance_list.append(performance)\n",
    "    mean_performance += performance\n",
    "    \n",
    "print(mean_performance/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mean_performance_list,bins=30)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Policy performance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have some time and want to convince yourself that because of the random nature of the enviroment different policies can lead to a good performance run the code below. This code will repeatively train and evaluate the policy performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (10):\n",
    "    # Training\n",
    "    policy_dict = {0:1, 1:2, 2:1, 3:0, 4:1, 6:1, 8:2, 9:0, 10:1, 13:2, 14:2} # initialize random policy\n",
    "    policy,greedy_policy,Q_sa,returns_Q_sa = MC_epsilon_greedy_every_visit(env,100000,policy_dict=policy_dict,gamma=0.95)\n",
    "    print(greedy_policy)\n",
    "    \n",
    "    # Testing\n",
    "    policy = lambda s: greedy_policy[s]\n",
    "    n = 300\n",
    "    mean_performance = 0\n",
    "    for i in range(n):\n",
    "        performance = test_performance(policy)\n",
    "        mean_performance += performance\n",
    "    \n",
    "    print(mean_performance/n)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating cumulative reward\n",
    "\n",
    "The cumulativ reward is defined as \n",
    "\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+2} + \\gamma^3 R_{t+3} + ...\n",
    "$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "Given a list of random returns $R$ calculate the cumulative return $G$ using one loop running normally through the list and one running reversed through the list. Whats the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_return_list = np.random.randint(20,size=(20))\n",
    "print(rand_return_list)\n",
    "gamma = 0.9\n",
    "\n",
    "#code here\n",
    "for r in reversed(rand_return_list):\n",
    "    # code here\n",
    "print(G)    \n",
    "\n",
    "# code here\n",
    "for r in rand_return_list:\n",
    "   # code here\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <a name=\"SUT18\"></a>[SUT18]\n",
    "        </td>\n",
    "        <td>\n",
    "            Richard S. Sutton and Andrew G. Barto, “Reinforcement Learning: An Introduction\n",
    "” ,2nd edition,2018. \n",
    "        </td>\n",
    "    </tr>\n",
    "   \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Licenses\n",
    "\n",
    "### Notebook License (CC-BY-SA 4.0)\n",
    "\n",
    "*The following license applies to the complete notebook, including code cells. It does however not apply to any referenced external media (e.g., images).*\n",
    "\n",
    "exercise-monte-carlo-frozenlake-gym\n",
    "\n",
    "Oliver Fischer\n",
    "\n",
    "is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/).<br/>\n",
    "Based on a work at https://gitlab.com/deep.TEACHING.\n",
    "\n",
    "\n",
    "### Code License (MIT)\n",
    "\n",
    "*The following license only applies to code cells of the notebook.*\n",
    "\n",
    "Copyright 2019  Oliver Fischer\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "deep_teaching_kernel",
   "language": "python",
   "name": "deep_teaching_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
